{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial on using the training pipeline for the event-based eye tracking challenge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse, json, os, mlflow\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from model.BaselineEyeTrackingModel import CNN_GRU\n",
    "from utils.training_utils import train_epoch, validate_epoch, top_k_checkpoints\n",
    "from utils.metrics import weighted_MSELoss\n",
    "from dataset import ThreeETplus_Eyetracking, ScaleLabel, NormalizeLabel, \\\n",
    "    TemporalSubsample, NormalizeLabel, SliceLongEventsToShort, \\\n",
    "    EventSlicesToVoxelGrid, SliceByTimeEventsTargets\n",
    "import tonic.transforms as transforms\n",
    "from tonic import SlicedDataset, DiskCachedDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Examplar config file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_file = 'sliced_baseline.json'\n",
    "with open(os.path.join('./configs', config_file), 'r') as f:\n",
    "    config = json.load(f)\n",
    "args = argparse.Namespace(**config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup mlflow tracking server (local)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='/home/zuowen_temp/Desktop/3et_final/challenge_demo_code/mlruns/780212397150265260', creation_time=1706705749274, experiment_id='780212397150265260', last_update_time=1706705749274, lifecycle_stage='active', name='trial_experiment', tags={}>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlflow.set_tracking_uri(args.mlflow_path)\n",
    "mlflow.set_experiment(experiment_name=args.experiment_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model and Optimizer Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your model, optimizer, and criterion\n",
    "model = eval(args.architecture)(args).to(args.device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=args.lr)\n",
    "\n",
    "if args.loss == \"mse\":\n",
    "    criterion = nn.MSELoss()\n",
    "elif args.loss == \"weighted_mse\":\n",
    "    criterion = weighted_MSELoss(weights=torch.tensor((args.sensor_width/args.sensor_height, 1)).to(args.device), \\\n",
    "                                    reduction='mean')\n",
    "else:\n",
    "    raise ValueError(\"Invalid loss name\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloding and Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we define the label transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "factor = args.spatial_factor # spatial downsample factor\n",
    "temp_subsample_factor = args.temporal_subsample_factor # downsampling original 100Hz label to 20Hz\n",
    "\n",
    "# The original labels are spatially downsampled with 'factor', downsampled to 20Hz, and normalized w.r.t width and height to [0,1]\n",
    "label_transform = transforms.Compose([\n",
    "    ScaleLabel(factor),\n",
    "    TemporalSubsample(temp_subsample_factor),\n",
    "    NormalizeLabel(pseudo_width=640*factor, pseudo_height=480*factor)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we define the raw event recording and label dataset, the raw events spatial coordinates are also spatially downsampled to 80x60 spatial resolutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_orig = ThreeETplus_Eyetracking(save_to=args.data_dir, split=\"train\", \\\n",
    "                transform=transforms.Downsample(spatial_factor=factor), \n",
    "                target_transform=label_transform)\n",
    "val_data_orig = ThreeETplus_Eyetracking(save_to=args.data_dir, split=\"val\", \\\n",
    "                transform=transforms.Downsample(spatial_factor=factor),\n",
    "                target_transform=label_transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we slice the event recordings into sub-sequences. The time-window is determined by the sequence length (train_length, val_length) and the temporal subsample factor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "slicing_time_window = args.train_length*int(10000/temp_subsample_factor) #microseconds\n",
    "train_stride_time = int(10000/temp_subsample_factor*args.train_stride) #microseconds\n",
    "\n",
    "train_slicer=SliceByTimeEventsTargets(slicing_time_window, overlap=slicing_time_window-train_stride_time, \\\n",
    "                seq_length=args.train_length, seq_stride=args.train_stride, include_incomplete=False)\n",
    "# the validation set is sliced to non-overlapping sequences\n",
    "val_slicer=SliceByTimeEventsTargets(slicing_time_window, overlap=0, \\\n",
    "                seq_length=args.val_length, seq_stride=args.val_stride, include_incomplete=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After slicing the raw event recordings into sub-sequences, we make each subsequences into your favorite event representation, in this case event voxel-\n",
    "\n",
    "You could also try other representations with the Tonic library easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_slicer_transform = transforms.Compose([\n",
    "    SliceLongEventsToShort(time_window=int(10000/temp_subsample_factor), overlap=0, include_incomplete=True),\n",
    "    EventSlicesToVoxelGrid(sensor_size=(int(640*factor), int(480*factor), 2), \\\n",
    "                            n_time_bins=args.n_time_bins, per_channel_normalize=args.voxel_grid_ch_normaization)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the Tonic SlicedDataset class to handle the collation of the sub-sequences into batches.\n",
    "\n",
    "The slicing indices will be cached to disk for faster slicing in the future, for the same slice parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata read from ./metadata/3et_train_tl_30_ts15_ch3/slice_metadata.h5.\n",
      "Metadata read from ./metadata/3et_val_vl_30_vs30_ch3/slice_metadata.h5.\n"
     ]
    }
   ],
   "source": [
    "train_data = SlicedDataset(train_data_orig, train_slicer, transform=post_slicer_transform, metadata_path=f\"./metadata/3et_train_tl_{args.train_length}_ts{args.train_stride}_ch{args.n_time_bins}\")\n",
    "val_data = SlicedDataset(val_data_orig, val_slicer, transform=post_slicer_transform, metadata_path=f\"./metadata/3et_val_vl_{args.val_length}_vs{args.val_stride}_ch{args.n_time_bins}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cache the preprocessed data to disk to speed up training. The first epoch will be slow, but the following epochs will be fast. This will consume certain disk space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = DiskCachedDataset(train_data, cache_path=f'./cached_dataset/train_tl_{args.train_length}_ts{args.train_stride}_ch{args.n_time_bins}')\n",
    "val_data = DiskCachedDataset(val_data, cache_path=f'./cached_dataset/val_vl_{args.val_length}_vs{args.val_stride}_ch{args.n_time_bins}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we wrap the dataset with pytorch dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_data, batch_size=args.batch_size, shuffle=True, \\\n",
    "                            num_workers=int(os.cpu_count()-2), pin_memory=True)\n",
    "val_loader = DataLoader(val_data, batch_size=args.batch_size, shuffle=False, \\\n",
    "                        num_workers=int(os.cpu_count()-2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Training Loop Functionalities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, val_loader, criterion, optimizer, args):\n",
    "    best_val_loss = float(\"inf\")\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(args.num_epochs):\n",
    "        model, train_loss, metrics = train_epoch(model, train_loader, criterion, optimizer, args)\n",
    "        mlflow.log_metric(\"train_loss\", train_loss, step=epoch)\n",
    "        mlflow.log_metrics(metrics['tr_p_acc_all'], step=epoch)\n",
    "        mlflow.log_metrics(metrics['tr_p_error_all'], step=epoch)\n",
    "\n",
    "        if args.val_interval > 0 and (epoch + 1) % args.val_interval == 0:\n",
    "            val_loss, val_metrics = validate_epoch(model, val_loader, criterion, args)\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                # save the new best model to MLflow artifact with 3 decimal places of validation loss in the file name\n",
    "                torch.save(model.state_dict(), os.path.join(mlflow.get_artifact_uri(), \\\n",
    "                            f\"model_best_ep{epoch}_val_loss_{val_loss:.4f}.pth\"))\n",
    "                \n",
    "                # DANGER Zone, this will delete files (checkpoints) in MLflow artifact\n",
    "                top_k_checkpoints(args, mlflow.get_artifact_uri())\n",
    "                \n",
    "            print(f\"[Validation] at Epoch {epoch+1}/{args.num_epochs}: Val Loss: {val_loss:.4f}\")\n",
    "            mlflow.log_metric(\"val_loss\", val_loss, step=epoch)\n",
    "            mlflow.log_metrics(val_metrics['val_p_acc_all'], step=epoch)\n",
    "            mlflow.log_metrics(val_metrics['val_p_error_all'], step=epoch)\n",
    "        # Print progress\n",
    "        print(f\"Epoch {epoch+1}/{args.num_epochs}: Train Loss: {train_loss:.4f}\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Training!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the major training loop including validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20: Train Loss: 0.9381\n",
      "[Validation] at Epoch 2/20: Val Loss: 0.0304\n",
      "Epoch 2/20: Train Loss: 0.0324\n",
      "Epoch 3/20: Train Loss: 0.0181\n",
      "[Validation] at Epoch 4/20: Val Loss: 0.0194\n",
      "Epoch 4/20: Train Loss: 0.0155\n",
      "Epoch 5/20: Train Loss: 0.0135\n",
      "[Validation] at Epoch 6/20: Val Loss: 0.0163\n",
      "Epoch 6/20: Train Loss: 0.0128\n",
      "Epoch 7/20: Train Loss: 0.0126\n",
      "[Validation] at Epoch 8/20: Val Loss: 0.0152\n",
      "Epoch 8/20: Train Loss: 0.0113\n",
      "Epoch 9/20: Train Loss: 0.0111\n",
      "[Validation] at Epoch 10/20: Val Loss: 0.0148\n",
      "Epoch 10/20: Train Loss: 0.0107\n",
      "Epoch 11/20: Train Loss: 0.0099\n",
      "[Validation] at Epoch 12/20: Val Loss: 0.0128\n",
      "Epoch 12/20: Train Loss: 0.0100\n",
      "Epoch 13/20: Train Loss: 0.0096\n",
      "[Validation] at Epoch 14/20: Val Loss: 0.0140\n",
      "Epoch 14/20: Train Loss: 0.0089\n",
      "Epoch 15/20: Train Loss: 0.0087\n",
      "[Validation] at Epoch 16/20: Val Loss: 0.0137\n",
      "Epoch 16/20: Train Loss: 0.0089\n",
      "Epoch 17/20: Train Loss: 0.0084\n",
      "[Validation] at Epoch 18/20: Val Loss: 0.0145\n",
      "Epoch 18/20: Train Loss: 0.0080\n",
      "Epoch 19/20: Train Loss: 0.0083\n",
      "[Validation] at Epoch 20/20: Val Loss: 0.0134\n",
      "Epoch 20/20: Train Loss: 0.0080\n"
     ]
    }
   ],
   "source": [
    "# Start MLflow run\n",
    "with mlflow.start_run(run_name=args.run_name):\n",
    "    # dump this training file to MLflow artifact\n",
    "    # mlflow.log_artifact(__file__) # Disabled for notebook, it is included in with the script\n",
    "\n",
    "    # Log all hyperparameters to MLflow\n",
    "    mlflow.log_params(vars(args))\n",
    "    # also dump the args to a JSON file in MLflow artifact\n",
    "    with open(os.path.join(mlflow.get_artifact_uri(), \"args.json\"), 'w') as f:\n",
    "        json.dump(vars(args), f)\n",
    "\n",
    "    # Train your model\n",
    "    model = train(model, train_loader, val_loader, criterion, optimizer, args)\n",
    "\n",
    "    # Save your model for the last epoch\n",
    "    torch.save(model.state_dict(), os.path.join(mlflow.get_artifact_uri(), f\"model_last_epoch{args.num_epochs}.pth\"))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "event_eyetracking",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
